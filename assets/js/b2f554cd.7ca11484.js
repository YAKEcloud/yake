"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[11477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"Gardener Hackathon 2023","metadata":{"permalink":"/blog/Gardener Hackathon 2023","editUrl":"https://github.com/23technologies/23ke-docs/tree/main/blog/2023-06-06-hackathon.md","source":"@site/blog/2023-06-06-hackathon.md","title":"Gardener Hackathon - May 2023","description":"TLDR;","date":"2023-06-06T00:00:00.000Z","formattedDate":"June 6, 2023","tags":[{"label":"gardener","permalink":"/blog/tags/gardener"},{"label":"hackathon","permalink":"/blog/tags/hackathon"}],"readingTime":1.77,"hasTruncateMarker":false,"authors":[{"name":"Jens Schneider","title":"software engineer","url":"https://github.com/jensac","imageURL":"https://github.com/jensac.png","key":"jensac"}],"frontMatter":{"slug":"Gardener Hackathon 2023","title":"Gardener Hackathon - May 2023","authors":"jensac","tags":["gardener","hackathon"]},"nextItem":{"title":"Build Cluster with CAPH - the challanges when installing 23KE","permalink":"/blog/build-cluster-with-caph"}},"content":"## TLDR;\\n\\nFrom May 22th until 26th, the colleagues from SAP, StackIT, x-cellent, and 23 Technologies met for another Gardener hackathon. One output is another [repository](https://github.com/gardener-community/hackathon) collecting the hackathon outputs. Go ahead and checkout the repo, for a concise summary of all past hackathons and information on future hackathons.\\n\\n## Another great experience with great achievements\\n\\nWhen we met on Monday 22th, we synchronized our expectations for the week in the first place. Almost everyone agreed that having a good time together belongs to the main expectations. A few days after the hackathon, I can definitely state that we had a good and productive time together. From the social perspective, we enjoyed the fruitful discussions during lunch or dinner. From the hacking perspective, we were really fascinated by the progress made with respect to some topics which have made it to the agenda for several times before:\\n\\n- Supporting pure IPv6 shoot clusters and\\n- Replacing the bash scripts for node provisioning with a golang-based approach.\\n\\nMoreover, we were working on a more research oriented topic dealing with the deployment of \\"masterful clusters\\" aka. \\"autonomous shoots\\". Even though the final concept for \\"gardener-like initial clusters\\" was not developed during the hackathon, the collected experience with respect to this challenging task is crucial for further steps.\\nBesides the bigger topics mentioned above, we brought the following task close to (or even in) production:\\n\\n- We moved the `machine-controller-manager` deployment responsibility to the `gardenlet`\\n- We introduced an `InternalSecret` resource in the Gardener API\\n- We replaced the `ShootState`s With data in backup buckets\\n- We found a concept for Garden cluster acccess for extensions in Seed clusters.\\n\\nOf course, there are still open questions and not every issue was solved during this short week. Therefore, we are happy that the colleagues from x-cellent opted for organizing the next Gardener hackathon in November/December 2023.\\n\\n## Conclusion\\n\\nOnce again, the Gardener hackathon was a great experience with great achievements for the overall project. The community work towards a \\"managed Kubernetes done right\\" service is still gathering pace which is forms a great basis for all future development."},{"id":"build-cluster-with-caph","metadata":{"permalink":"/blog/build-cluster-with-caph","editUrl":"https://github.com/23technologies/23ke-docs/tree/main/blog/2023-05-12-build-cluster-with-caph.md","source":"@site/blog/2023-05-12-build-cluster-with-caph.md","title":"Build Cluster with CAPH - the challanges when installing 23KE","description":"TLDR;","date":"2023-05-12T00:00:00.000Z","formattedDate":"May 12, 2023","tags":[{"label":"gardener","permalink":"/blog/tags/gardener"},{"label":"caph","permalink":"/blog/tags/caph"},{"label":"k8s","permalink":"/blog/tags/k-8-s"}],"readingTime":6.145,"hasTruncateMarker":false,"authors":[{"name":"Marius Wernicke","title":"DevOps Engineer","url":"https://github.com/rhizoet","imageURL":"https://github.com/rhizoet.png","key":"rhizoet"}],"frontMatter":{"slug":"build-cluster-with-caph","title":"Build Cluster with CAPH - the challanges when installing 23KE","authors":"rhizoet","tags":["gardener","caph","k8s"]},"prevItem":{"title":"Gardener Hackathon - May 2023","permalink":"/blog/Gardener Hackathon 2023"},"nextItem":{"title":"Hack-The-Garden - the Remote Hackathon Experience","permalink":"/blog/hack-the-garden"}},"content":"## TLDR;\\nWe recently built new Kubernetes clusters on Hetzner Cloud. We had several challenges to get the cluster up and running.\\n\\nThis started with the selection of the correct Kubernetes version, the CNI solution and the actual deployment of 23KE. Spoiler: We had to change the CNI solution and reset containerd.\\n\\nIf these instructions in this blog post are followed, you can build a working Gardener cluster. \\n\\n**Table of Contents**\\n- [TLDR;](#tldr)\\n- [Introduction](#introduction)\\n- [Requirements](#requirements)\\n- [Setup the management cluster](#setup-the-management-cluster)\\n  - [Modifications](#modifications)\\n- [Setup the worker cluster](#setup-the-worker-cluster)\\n- [Install of 23KE](#install-of-23ke)\\n- [Summary](#summary)\\n\\n## Introduction\\nIn times of rising costs and the reduction of the CO<sub>2</sub> footprint in all areas of life, we also wanted to reduce these sustainably in our day-to-day operations.\\n\\nWe had been running okeanos.dev on a managed kubernetes cluster on Azure. This is very expensive so we wanted to minimize these costs. In this case, the European cloud from Hetzner was the obvious choice. The following question was how we could best build a k8s cluster there.\\n\\nAfter some research, ClusterAPI provider for Hetzner (CAPH) from [Syself](https://syself.com/) turned out to be the optimal solution.\\n\\nWhen testing the provider, we had to overcome a few challenges, which we would like to discuss in the following.\\n\\n## Requirements\\nYou need to install some basic tools to work with CAPH and Gardener. It makes sense to set up a management vm (on Hetzner) running a kind cluster and on it the management cluster. \\n\\n* [docker-engine](https://docs.docker.com/engine/install/)\\n* [kind](https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries)\\n* [clusterctl](https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl)\\n* [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)\\n* [helm](https://helm.sh/docs/intro/install/)\\n* [flux](https://fluxcd.io/flux/installation/)\\n* [k9s](https://k9scli.io/topics/install/)\\n\\n## Setup the management cluster\\nTo start, let\'s create a kind cluster with a customized Kubernetes version (Currently it is important that the kubernetes version is `>1.26`)\\n\\n```shell\\nkind create cluster -n my-cluster --image=kindest/node:v1.25.9\\n```\\n\\nWhen the cluster is up, [initialize the management cluster](https://github.com/syself/cluster-api-provider-hetzner/blob/main/docs/topics/preparation.md) with CAPH\\n\\n```shell\\nclusterctl init --core cluster-api --bootstrap kubeadm --control-plane kubeadm --infrastructure hetzner\\n```\\nand export your environment variables\\n```shell\\nexport HCLOUD_SSH_KEY=\\"MY_SSH_KEY\\" \\\\\\nexport CLUSTER_NAME=\\"my-cluster\\" \\\\\\nexport HCLOUD_REGION=\\"nbg1\\" \\\\\\nexport CONTROL_PLANE_MACHINE_COUNT=3 \\\\\\nexport WORKER_MACHINE_COUNT=3 \\\\\\nexport KUBERNETES_VERSION=1.25.9 \\\\\\nexport HCLOUD_CONTROL_PLANE_MACHINE_TYPE=cpx31 \\\\\\nexport HCLOUD_WORKER_MACHINE_TYPE=cpx41 \\\\\\nexport HCLOUD_TOKEN=\\"YOUR_HCLOUD_TOKEN_HERE\\"\\n```\\nThe region can be for sure anything else like `hel1`.\\n\\nTo be able to build the machines a secret must be created:\\n```shell\\nkubectl create secret generic hetzner --from-literal=hcloud=$HCLOUD_TOKEN\\n```\\n\\nYou can also still build yourself a customized node image, but we didn\'t do that. We used the Ubuntu 22.04 image, which is available from Hetzner.\\n\\nNow let\'s create the `my-cluster.yaml` with the private network flavor as in the [quickstart-guide](https://github.com/syself/cluster-api-provider-hetzner/blob/main/docs/topics/quickstart.md):\\n```shell\\nclusterctl generate cluster my-cluster --kubernetes-version v1.25.9 --control-plane-machine-count=3 --worker-machine-count=3 --flavor hcloud-network > my-cluster.yaml\\n```\\n\\n### Modifications\\n\\nAfter creation you need to modify the `my-cluster.yaml`. Remove the following blocks (it gives two of them) because we need to reset the containerd config\\n```diff\\n- - content: |\\n-     version = 2\\n-     [plugins.\\"io.containerd.grpc.v1.cri\\".containerd.runtimes.runc]\\n-       runtime_type = \\"io.containerd.runc.v2\\"\\n-     [plugins.\\"io.containerd.grpc.v1.cri\\".containerd.runtimes.runc.options]\\n-       SystemdCgroup = true\\n-     [plugins.\\"io.containerd.grpc.v1.cri\\".containerd.runtimes.crun]\\n-       runtime_type = \\"io.containerd.runc.v2\\"\\n-     [plugins.\\"io.containerd.grpc.v1.cri\\".containerd.runtimes.crun.options]\\n-       BinaryName = \\"crun\\"\\n-       Root = \\"/usr/local/sbin\\"\\n-       SystemdCgroup = true\\n-     [plugins.\\"io.containerd.grpc.v1.cri\\".containerd]\\n-       default_runtime_name = \\"crun\\"\\n-     [plugins.\\"io.containerd.runtime.v1.linux\\"]\\n-       runtime = \\"crun\\"\\n-       runtime_root = \\"/usr/local/sbin\\"\\n-   owner: root:root\\n-   path: /etc/containerd/config.toml\\n-   permissions: \\"0744\\"\\n```\\nand add\\n```diff\\n+ - mkdir /etc containerd\\n+ - containerd config default > /etc/containerd/confi.toml\\n+ - sed -i \'s/SystemdCgroup = false/SystemdCgroup = true/g\' /etc/containerd/config.toml\\n  - systemctl daemon-reload && systemctl enable containerd && systemctl start containerd\\n+ - sysctl fs.inotify.max_user_instances=8192\\n+ - sysctl fs.inotify.max_user_watches=524288\\n```\\n\\nThe containerd-config is missing some options. In addition, the `SystemdCgroup` must be set to `true` and the `inotify` settings needs to be increased. Only then the `vpn-seed-server` starts, which is created when a shoot is created.\\n\\nWhen you have finished the modification, you can start building the worker cluster by appling the modified file on the management cluster\\n\\n```shell\\nkubectl apply -f my-cluster.yaml\\n```\\nWith `watch clusterctl describe cluster my-cluster` you can watch one control-plane and three workers are being built.\\n\\n## Setup the worker cluster\\n\\n> **_NOTE:_** From now on you should make sure that you are working on the newly built cluster. Make sure that you have exported the Kubeconfig for the worker cluster!\\n\\nGet the kubeconfig of the worker cluster\\n```shell\\nclusterctl get kubeconfig my-cluster > /path/to/my/worker-cluster.kc\\n```\\nand export it to work on the cluster (you can do this on your local machine. You don\'t need to work anymore on the management vm):\\n```shell\\nexport KUBECONFIG=/path/to/my/worker-cluster.kc\\n```\\n\\nAt the point of \\"Deploy a CNI solution\\" keep in mind not to install cilium like in the guide but [calico](https://docs.tigera.io/calico/latest/getting-started/kubernetes/helm). We had a lot of trouble with cilium.\\n```shell\\nhelm repo add projectcalico https://docs.tigera.io/calico/charts\\nkubectl create namespace tigera-operator\\nhelm install calico projectcalico/tigera-operator --version v3.25.1 --namespace tigera-operator\\n```\\nWait till calico is ready.\\n\\nNow deploy the CCM for hcloud only. Set `privateNetwork.enabled=true` because we need a private network to function properly:\\n```shell\\nhelm repo add syself https://charts.syself.com\\nhelm upgrade --install ccm syself/ccm-hcloud --version 1.0.11 \\\\\\n  --namespace kube-system \\\\\\n  --set secret.name=hetzner \\\\\\n  --set secret.tokenKeyName=hcloud \\\\\\n  --set privateNetwork.enabled=true\\n```\\n\\nAt the end we need a CSI to build volumes on hcloud:\\n```shell\\ncat << EOF > csi-values.yaml\\nstorageClasses:\\n  - name: hcloud-volumes\\n    defaultStorageClass: true\\n    reclaimPolicy: Retain\\nEOF\\n\\nhelm upgrade --install csi syself/csi-hcloud --version 0.2.0 \\\\\\n  --namespace kube-system -f csi-values.yaml\\n```\\n\\nThe remaining control planes should now be built and added to the cluster. Wait until the entire cluster has `Ready` status before proceeding.\\n\\n> **_NOTE:_** If you want to transform the workload cluster into a management cluster, you need to do the \\"move\\"-steps in the guide. But its recommended to have a management vm with kind installed and hold there the management cluster.\\n\\n## Install of 23KE\\n\\nIf you plan to install 23KE, you need to keep in mind that there are a few customizations that need to be made for hcloud.\\n\\nIf you have installed 23KE and the data from the installation is in a repository, the following file need to be adjusted.\\n\\nAdd the following lines in `gardenlet-values.yaml`\\n```diff\\nsettings:\\n+ loadBalancerServices:\\n+   annotations:\\n+     load-balancer.hetzner.cloud/location: nbg1\\n+     load-balancer.hetzner.cloud/ipv6-disabled: \\"true\\"\\n+     load-balancer.hetzner.cloud/disable-private-ingress: \\"true\\"\\n```\\n\\nCommit and push your changes. You can execute a `flux reconcile source git 23ke-config` to speed up the things.\\n\\nIf any ingress won\'t get a public IP, you can add some annotations manualy e.g. for `nginx-ingress-controller`:\\n```shell\\nkubectl annotate svc -n garden nginx-ingress-controller load-balancer.hetzner.cloud/location=nbg1 \\\\\\n  load-balancer.hetzner.cloud/ipv6-disabled=true \\\\\\n  load-balancer.hetzner.cloud/disable-private-ingress=true\\n```\\n\\nNow all things should come up as well as e.g. the dashboard should be accessible. If something is not running smoothly, the cluster can be inspected with k9s and you have an easy overview of what the problem is.\\n\\nIn the next steps, secrets can be added to connect to a public or private cloud. With these and a cloudprofile it is then possible to build and run shoots (k8s clusters).\\n\\n## Summary\\n\\nWith all these steps, it is possible for you to build a functional Gardener cluster on Hetzner Cloud. With this you can run a low cost K8s cluster and run what you want on it. Whereas 23KE is already very cool.\\n\\nThis setup has allowed us to drastically reduce the cost of a fully functional Gardener. As a pleasant side effect, we are no longer dependent on a cloud in the US, but now run Gardener in a data center in Germany with a German operator that is GDPR compliant and uses green electricity (good for our CO<sub>2</sub> footprint)."},{"id":"hack-the-garden","metadata":{"permalink":"/blog/hack-the-garden","editUrl":"https://github.com/23technologies/23ke-docs/tree/main/blog/2022-10-06-hack-the-garden.md","source":"@site/blog/2022-10-06-hack-the-garden.md","title":"Hack-The-Garden - the Remote Hackathon Experience","description":"Thanks to everyone","date":"2022-10-06T00:00:00.000Z","formattedDate":"October 6, 2022","tags":[{"label":"gardener","permalink":"/blog/tags/gardener"},{"label":"gardener-extensions","permalink":"/blog/tags/gardener-extensions"}],"readingTime":1.185,"hasTruncateMarker":false,"authors":[{"name":"Jens Schneider","title":"software engineer","url":"https://github.com/jensac","imageURL":"https://github.com/jensac.png","key":"jensac"}],"frontMatter":{"slug":"hack-the-garden","title":"Hack-The-Garden - the Remote Hackathon Experience","authors":"jensac","tags":["gardener","gardener-extensions"]},"prevItem":{"title":"Build Cluster with CAPH - the challanges when installing 23KE","permalink":"/blog/build-cluster-with-caph"},"nextItem":{"title":"A Single Point of Truth for Gardener Helm Charts","permalink":"/blog/gardener-chart-releaser"}},"content":"## Thanks to everyone\\nIn the last week of September 2022, I participated in the [Gardener Hackathon](https://metal-stack.io/blog/2022/10/hack-the-garden/). Unfortunately, I need to attend the event remotely, as I caught Covid the week before the event. Therefore, I want to thank everyone making this experience possible for me, anyway.\\n\\n## Even remote hacking together is great\\nAs [Tim](https://github.com/timebertt/timebertt) also participated remotely, we formed a remote hacking team contributing to the development of the Gardener [extension-registry-cache](https://github.com/gardener/gardener-extension-registry-cache). Of course, we were in touch with the on-site contributors via video calls which laid the foundation for three highly productive hacking days. Right after tying down a common todo list, we distributed the workload to the on-site and remote teams, and started hacking. From time to time, we held synchronization meetings, so that everyone was up-to-date and the current state of work was not only reflected in commits, branches, and pull request but also communicated to everyone in the team.\\n\\nBeyond the internal communication within the team, Tim and [Rafael](https://github.com/rfranzke) organized a demo session on day 2. It was really amazing to see the progress made with respect to the various topics covered by the hackathon.\\n\\n## Conclusion\\nClearly, the hackathon was a great event, and even hacking together remotely was a great experience. Of course, the social aspect of working together on-site cannot be mimicked. Therefore, I am already looking forward to the next Gardener hackathon which I hopefully can attend on-site."},{"id":"gardener-chart-releaser","metadata":{"permalink":"/blog/gardener-chart-releaser","editUrl":"https://github.com/23technologies/23ke-docs/tree/main/blog/2022-09-19-gardener-chart-releaser.md","source":"@site/blog/2022-09-19-gardener-chart-releaser.md","title":"A Single Point of Truth for Gardener Helm Charts","description":"TLDR;","date":"2022-09-19T00:00:00.000Z","formattedDate":"September 19, 2022","tags":[{"label":"gardener","permalink":"/blog/tags/gardener"},{"label":"gardener-community","permalink":"/blog/tags/gardener-community"}],"readingTime":5.08,"hasTruncateMarker":false,"authors":[{"name":"Jens Schneider","title":"software engineer","url":"https://github.com/jensac","imageURL":"https://github.com/jensac.png","key":"jensac"},{"name":"Malte M\xfcnch","url":"https://github.com/mxmxchere","imageURL":"https://github.com/mxmxchere.png","key":"mxmxchere"}],"frontMatter":{"slug":"gardener-chart-releaser","title":"A Single Point of Truth for Gardener Helm Charts","authors":["jensac","mxmxchere"],"tags":["gardener","gardener-community"]},"prevItem":{"title":"Hack-The-Garden - the Remote Hackathon Experience","permalink":"/blog/hack-the-garden"},"nextItem":{"title":"Getting started with Gardener extension development","permalink":"/blog/gardener-ext-dev"}},"content":"## TLDR;\\nRecently, we consolidated Gardener related Helm charts in a helm repository hosted on GitHub pages of the [gardener-charts](https://github.com/gardener-community/gardener-charts) repository.\\nFor this purpose we implemented a custom chart release bot - the [gardener-chart-releaser](https://github.com/gardener-community/gardener-chart-releaser).\\nKeep on reading to find out more.\\n\\n\x3c!-- markdown-toc start - Don\'t edit this section. Run M-x markdown-toc-refresh-toc --\x3e\\n**Table of Contents**\\n\\n- [TLDR;](#tldr)\\n- [Introduction](#introduction)\\n- [The Gardener Chart Releaser](#the-gardener-chart-releaser)\\n\\t- [Export the charts to a local directory](#export-the-charts-to-a-local-directory)\\n\\t- [Update all version field to the latest version](#update-all-version-field-to-the-latest-version)\\n- [Handling Gardener extensions](#handling-gardener-extensions)\\n- [Running the release process nightly](#running-the-release-process-nightly)\\n- [Summary](#summary)\\n\\n\x3c!-- markdown-toc end --\x3e\\n\\n## Introduction\\nInstalling [Gardener](https://github.com/gardener/gardener) is a complicated process, even though the [garden-setup](https://github.com/gardener/garden-setup) installer is provided in the same GitHub organization space. One of the reasons is that the Gardener related Helm charts are spread over multiple repositories.\\nConsequently, other Helm chart-based installers like [Schrodit\'s](https://github.com/schrodit/gardener-installation) [gardener-installation](https://github.com/schrodit/gardener-installation) popped up.\\nMoreover, we consolidated gardener related Helm charts in the [23ke-charts](https://github.com/23technologies/23ke-charts) repository by a simple Python script, and developed a very [basic installation approach](https://github.com/23technologies/23ke-charts/tree/main/hack/gardener-from-helmcharts) based on these charts.\\nThe chart collection was released in a Helm repository hosted on GitHub pages by helm\'s [chart-releaser](https://github.com/helm/chart-releaser).\\nWith the consolidation of the Gardener charts, we faced the issue that the collected charts need to be kept up to date somehow.\\nThere our journey begins, and we started to keep track of the upstream charts with the help of [renovate](https://github.com/renovatebot/renovate).\\nHowever, renovate is designed to keep dependencies up to date and finds its limitations, when it comes to tracking multiple versions of the same piece of software.\\nFirst, we tried to find a workaround by tracking three branches for the last three minor versions and shifted the latest branch, as soon as a new minor release appeared.\\nEven though this approach could potentially succeed, we were running into issues from time to time due to missing automatic merges, or failures in the branch shift routine.\\nAs a consequence, we decided to build our own tracking tool, the [gardener-chart-releaser](https://github.com/gardener-community/gardener-chart-releaser).\\n\\n## The Gardener Chart Releaser\\nAs already stated above, we wanted to keep track of the last three minor versions of all Gardener related Helm charts, and release these charts in a single helm repository.\\nIn order to achieve this, we needed to make some decisions.\\nFirst, we needed to drop our old Python-based Helm chart import script, as working with helm charts in code is way easier, when using the Go-based [helm packages](https://github.com/helm/helm/tree/main/pkg) directly. Further, helm\'s [chart-releaser](https://github.com/helm/chart-releaser) is written in Go and there exists a solid implementation of a git and GitHub module in Go.\\nSo, we reimplemented our chart import and release functionality in Go with a view to tracking the last three minor releases.\\nAnother design goal was to keep the tool simple, especially from the user point of view.\\nAs of now, the user only needs to worry about a configuration file in yaml format.\\nConsider the following example:\\n```\\n# contents of config.yaml\\ndestination:\\n\\towner: gardener-community\\n\\trepo: gardener-charts\\nsources:\\n\\t- name: gardener-controlplane\\n\\t  version: v1.53.0\\n\\t  repo: gardener/gardener\\n\\t  charts:\\n\\t\\t- charts/gardener/controlplane\\n  - ...\\n```\\nThe `destination` map defines a GitHub owner and repository, where the collected charts are released.\\nUnder `sources` a list of source Helm charts is provided by an `owner/repo` entry and a list of paths pointing to the charts to be released.\\nWith a valid `config.yaml` a user can simply run\\n```shell\\nexport GITHUB_TOKEN=....\\ngardener-chart-releaser update\\n```\\nand the configured charts will be collected and released.\\nNote that the `version` field is ignored for the actual release process, as we want to track several versions.\\nHowever, the `version` field has its own reasoning. Keep on reading ;-)\\n\\n### Export the charts to a local directory\\nJust by collecting and releasing charts to a GitHub repository you won\'t get to see the charts\' contents at all.\\nBut what if you want to work with the charts itself in a local development scenario.\\nFor this purpose, you can export the charts to a local directory instead of releasing them to a remote repo.\\nJust call\\n```shell\\ngardener-chart-releaser export\\n```\\nand the charts will be exported to a local `./charts` directory.\\nIn this case, the `version` field in `config.yaml` defines the version to be exported.\\n\\n### Update all version field to the latest version\\nAs the entire Gardener ecosystem is moving quickly, your `config.yaml` will be outdated soon.\\nIn order to avoid manual updates of the `version` fields, we introduced another command called `fetchLatestVersions`.\\nIf you run\\n```shell\\ngardener-chart-releaser fetchLatestVersions\\n```\\nyour `config.yaml` will be updated, so that you will find the versions of the latest upstream releases in the file.\\nOf course, it only makes sense to run this before a local export to make sure that the most recent versions of charts are exported.\\n\\n## Handling Gardener extensions\\nYou might be wondering how Gardener extensions are managed in this approach, as these are not provided as Helm charts upstream.\\nRemember that we wanted to build a single point of truth for a Gardener provisioning, and consequently the gardener-chart-releaser also packages Gardener extensions as charts.\\nFor each entry like e.g.\\n```yaml\\nsources:\\n  - name: runtime-gvisor\\n\\tversion: v0.5.1\\n\\trepo: gardener/gardener-extension-runtime-gvisor\\n\\tcharts:\\n\\t  - controller-registration\\n```\\nin the configuration file, it will generate a Helm chart for the specified extension and release it the same way as the Gardener core charts.\\nFurthermore, this approach provides the opportunity to release charts for the extension itself (i.e. controllerRegistration and controllerDeployment) and the charts for the admission controllers as sub-charts.\\nFor instance\\n```yaml\\nsources:\\n  - name: provider-azure\\n\\tversion: v1.29.0\\n\\trepo: gardener/gardener-extension-provider-azure\\n\\tcharts:\\n\\t  - controller-registration\\n\\t  - charts/gardener-extension-admission-azure\\n```\\nwill package a top-level chart called `provider-azure` with sub-charts for the extension controller and admission controller, respectively.\\n\\n## Running the release process nightly\\nAs we want to be as transparent as possible, we set up a GitHub [action](https://github.com/gardener-community/gardener-charts/actions), so that the chart-releaser is run nightly.\\nThis will ensure that we do not miss any important upstream change and the Helm repository is always up to date.\\n\\n## Summary\\nThe gardener-chart-releaser enables a single point of truth for Gardener related Helm charts and could be a good starting point for custom Gardener installation routines."},{"id":"gardener-ext-dev","metadata":{"permalink":"/blog/gardener-ext-dev","editUrl":"https://github.com/23technologies/23ke-docs/tree/main/blog/2022-06-08-gardener-ext-dev.md","source":"@site/blog/2022-06-08-gardener-ext-dev.md","title":"Getting started with Gardener extension development","description":"TLDR;","date":"2022-06-08T00:00:00.000Z","formattedDate":"June 8, 2022","tags":[{"label":"gardener","permalink":"/blog/tags/gardener"},{"label":"gardener-extensions","permalink":"/blog/tags/gardener-extensions"}],"readingTime":11.195,"hasTruncateMarker":false,"authors":[{"name":"Jens Schneider","title":"software engineer","url":"https://github.com/jensac","imageURL":"https://github.com/jensac.png","key":"jensac"}],"frontMatter":{"slug":"gardener-ext-dev","title":"Getting started with Gardener extension development","authors":"jensac","tags":["gardener","gardener-extensions"]},"prevItem":{"title":"A Single Point of Truth for Gardener Helm Charts","permalink":"/blog/gardener-chart-releaser"},"nextItem":{"title":"A Gardener Extension for universal Shoot Configuration","permalink":"/blog/gardener-ext-shoot-flux"}},"content":"## TLDR;\\nRecently, we developed the [gardener-extension-mwe](https://github.com/23technologies/gardener-extension-mwe), which serves as a minimal working example for Gardener extensions.\\nIf you are only interested in the code, go and checkout the repository on github.\\nIf you want to learn more, keep on reading.\\n\\n\x3c!-- markdown-toc start - Don\'t edit this section. Run M-x markdown-toc-refresh-toc --\x3e\\n**Table of Contents**\\n\\n- [TLDR;](#tldr)\\n- [Introduction](#introduction)\\n- [Basic workflows](#basic-workflows)\\n- [The mininal working example](#the-mininal-working-example)\\n    - [Rapid prototyping on a Kubernetes cluster (tested with version 1.22.6)](#rapid-prototyping-on-a-kubernetes-cluster-tested-with-version-1226)\\n    - [Development in a real Gardener environment](#development-in-a-real-gardener-environment)\\n- [Last words](#last-words)\\n\\n\x3c!-- markdown-toc end --\x3e\\n\\n## Introduction\\nStarting the development of new [Gardener](https://gardener.cloud/) extensions can be challenging.\\nAs the [Gardener documentation](https://gardener.cloud/docs/) is fairly complex and driven by the history of the project, getting into the overall concepts is not easy.\\nMoreover, code examples for Gardener extensions reside in separate Git repositories lacking documentation.\\nHowever, early in March 2022 the [gardener-extension-shoot-networking-filter](https://github.com/gardener/gardener-extension-shoot-networking-filter) was published, which comes at a more beginner friendly level than the e.g. the cloud-provider extensions.\\nIn particular, it extends `Shoot` clusters by the use of [managed resources](https://gardener.cloud/docs/gardener/concepts/resource-manager/#managedresource-controller), which might be more straight-forward than the interaction with a cloud service provider as performed by e.g. the [gardener-extension-provider-aws](https://github.com/gardener/gardener-extension-provider-aws).\\nThus, [gardener-extension-shoot-networking-filter](https://github.com/gardener/gardener-extension-shoot-networking-filter) provides a reasonable starting point for new developments, which target at automated deployments into `Shoot` clusters.\\n\\nHowever, going beyond the identification of a starting point, it makes sense to take a closer look at the concepts for extension development.\\nIn the [extension directory](https://github.com/gardener/gardener/tree/master/extensions) of the Gardener Git repository, we find several Go-packages defining interfaces, which can be implemented by extension controllers.\\nPut simply, we can search for files matching `pkg/controller/*/actuator.go`, in order to find interfaces for controllers acting on the corresponding resources.\\nE.g., if our controller defines a (golang)-type -- called `actuator` -- implementing the interface defined in `pkg/controller/extension/actuator.go`, the controller will reconcile resources of type `*extensionsv1alpha1.Extension`.\\nConsequently, the controller will take care for all the steps we define in the `Reconcile` method of the `actuator`, when `Extension` resources are updated.\\nOf course, there might be more complex scenarios where reconciling `Extension` resources would be insufficient.\\nIn these cases, other interfaces like e.g. defined in `pkg/controller/infrastructure/actuator.go` would need to be implemented.\\nHowever, these cases lie beyond the scope of this blog post.\\n\\nNext, we will dive into some basic workflows for Gardener extension development.\\n\\n## Basic workflows\\nIn software engineering, it is reasonable to develop on a local machine with a controllable toolchain.\\nAs already mentioned above, Gardener extensions are implemented in [Go](https://go.dev/).\\nTherefore, let\'s identify a few requirements for the development:\\n- An installation of Go\\n- A text editor, which (optionally) supports [gopls](https://pkg.go.dev/golang.org/x/tools/gopls)\\n- A Go debugger, which is most likely to be [Delve](https://github.com/go-delve/delve)\\n- A Gardener development environment. This can be setup by\\n\\t- [Running Gardener locally](https://gardener.cloud/docs/gardener/development/getting_started_locally/) (also checkout [#5548](https://github.com/gardener/gardener/issues/5548), if you are running on Linux)\\n\\t- Setting up a development Gardener on some cloud infrastructure. This definitely comes closer to the real world scenario your extension will eventually live in.\\nThe block diagram below depicts the overall setup including the requirements from above.\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502    Development Computer                      \u2502\\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n\u2502                                              \u2502\\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\\n\u2502 \u2502             - Your toolchain             \u2502 \u2502\\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\\n\u2502                                              \u2502\\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\\n\u2502     \u2502Kubeconfigs \u2502        \u2502Your code     \u2502   \u2502\\n\u2502     \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\\n\u2502        \u2502      \u2502                    \u2502         \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n         \u2502      \u2502                    \u2502\\n         \u2502      \u2502apply               \u2502\\n  apply  \u2502      \u2502resources           \u2502reconcile\\nresources\u2502      \u2502                    \u2502resources\\n         \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\\n         \u2502                         \u2502 \u2502\\n         \u2502                         \u2502 \u2502\\n         \u25bc                         \u25bc \u25bc\\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n   \u2502 Garden Cluster  \u2502       \u2502 Seed Cluster    \u2502\\n   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n   \u2502- Project        \u2502       \u2502- Extension      \u2502\\n   \u2502- Seed           \u2502       \u2502- Controller     \u2502\\n   \u2502- Shoot          \u2502       \u2502- ...            \u2502\\n   \u2502- ...            \u2502       \u2502                 \u2502\\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\nAs you can see, the code for the extension controller is running on your local development machine, and reconciles resources, such as `Extension` resources, in the `Seed` cluster.\\nOf course, you will not have dedicated clusters for the `Garden` cluster and `Seed` cluster, when running Gardener locally.\\nHowever, the overall buidling blocks stay conceptually the same.\\nOnce these requirements are met, you are good to go for your first steps with Gardener extensions.\\nWait! I have to setup an entire Gardener, if I want to rapidly prototype an extension controller?\\nYes and No. Depending on your development/test case, it might be reasonable to \\"fake\\" a Gardener environment on a vanilla Kubernetes cluster. We will get to this development case below.\\nFor rock solid testing, however, you most probably need a real world Gardener environment.\\n\\n## The mininal working example\\n\\nAs of May 2022, we provide a Minimal Working Example (MWE) for gardener extensions.\\nOf course, this example did not come out of nowhere.\\nTherefore, we review the development process and break the example down to its components in the following:\\nTaking a look at other extensions, it is observed that we need some boiler plate code for running our controller, so that it works together with all the other Gardener components.\\nFor the MWE, we collected the relevant files and adjusted them to our needs.\\nThus, we can have a look at the `cmd` directory of the [gardener-extension-mwe](https://github.com/23technologies/gardener-extension-mwe) and find a simple structure with 3 files, which are responsible for starting our controller and ensuring that it acts on the defined resources.\\n```\\ncmd\\n\u2514\u2500\u2500 gardener-extension-mwe\\n    \u251c\u2500\u2500 app\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 app.go\\n    \u2502\xa0\xa0 \u2514\u2500\u2500 options.go\\n    \u2514\u2500\u2500 main.go\\n```\\nIf you want to start the development of a new extension, you do not need to worry too much about these files.\\nMost probably, you can simple copy them over, and adjust some variables to your needs.\\nActually, we also copied these files from the [gardener-extension-shoot-networking-filter](https://github.com/gardener/gardener-extension-shoot-networking-filter) and adjusted them to the needs of the MWE.\\nGiven that we have the boilerplate code in the `cmd` directory available now, we can go ahead and define a type which implements an `actuator` interface.\\nFor this, we need the files in the `pkg` directory.\\nLets take a look at the structure:\\n```\\npkg\\n\u2514\u2500\u2500 controller\\n\\t\u2514\u2500\u2500 lifecycle\\n\\t\\t\u251c\u2500\u2500 actuator.go\\n\\t\\t\u2514\u2500\u2500 add.go\\n```\\nAlso here, we find only two files, and the implementation of the interface is located in [`actuator.go`](https://github.com/23technologies/gardener-extension-mwe/blob/main/pkg/controller/lifecycle/actuator.go).\\nThis is the place where most of the magic of your new controller happens.\\nIn the case of the MWE, the actuator will only output logs, when `Extension` resources are reconciled.\\nObviously, all code is written in Go and consequently we will also need to pull in some dependencies.\\nFor this, we need the files `go.mod` and `go.sum`.\\nTypically, the source code of the dependencies is also committed to the repository, which comes at advantages and downsides.\\nThe main advantage is that all code needed for building an application is available in the repository.\\nOn the other hand, committing several 1000s lines of code during vendoring clutters the commit history of the repository.\\nTherefore, we only provide the files mentioned above in the [initial commit](https://github.com/23technologies/gardener-extension-mwe/commit/455c9c76876161bf8d5197e1330a9fc28f825baa) of the MWE and perform the vendoring (by running `go mod vendor`) in [another commit](https://github.com/23technologies/gardener-extension-mwe/commit/3c238bdcc5697392d567d5b6e5f2cf6126b3c756).\\nAt this state of the repository, we can already start and go the first steps with our new controller in a vanilla Kubernetes cluster.\\n\\n### Rapid prototyping on a Kubernetes cluster (tested with version 1.22.6)\\nAssuming you have read the [basic workflows](#basic-workflows) section, we are ready to dive into the exemplary development techniques.\\nSo let\'s fetch the code and setup the repository:\\n``` shell\\ngit clone https://github.com/23technologies/gardener-extension-mwe.git\\ncd gardener-extension-mwe\\ngit checkout 3c238bd  # checkout the commit containing first vendoring\\nmkdir dev\\ncp PATH-TO/KUBECONFIG.yaml dev/kubeconfig.yaml\\n```\\nNow, we can already start our controller and should get some output showing that it was started:\\n``` shell\\ngo run ./cmd/gardener-extension-mwe --kubeconfig=dev/kubeconfig.yaml  --ignore-operation-annotation=true --leader-election=false\\n```\\nHowever, we will not observe any other output, since the controller is still freewheeling.\\nRemember, reconciliation will be triggered on `Extension` resources.\\nAs our vanilla Kubernetes cluster does not know anything about `Extension` resources yet, we will have to \\"fake\\" the Gardener environment.\\nIn other Gardener extensions, we find resources for a \\"fake\\" Gardener setup in the `example` directory.\\nTherefore, we prepared the `example` directory in [another commit](https://github.com/23technologies/gardener-extension-mwe/commit/50f7136330e114ec2f795f3e30a756381dd4cbc6).\\nLet\'s check it out:\\nOpen a new terminal pane and navigate to your repository and go for\\n``` shell\\ngit checkout 50f7136 # this commit entails the example directory\\nexport KUBECONFIG=dev/kubeconfig.yaml\\nkubectl apply -f example/10-fake-shoot-controlplane.yaml\\nkubectl apply -f example/20-crd-cluster.yaml\\nkubectl apply -f example/20-crd-extension.yaml\\nkubectl apply -f example/30-cluster.yaml\\n```\\nNow, the cluster simulates a Gardener environment and we can apply an `Extension` resource:\\n``` shell\\nkubectl apply -f example/40-extension.yaml\\n```\\nTake another look at the terminal running our controller now.\\nIt should have logged a \\"Hello World\\" message.\\nOf course, we can also delete the `Extension` resource again and the controller will tell us that the `Delete` method was called.\\n``` shell\\nkubectl delete -f example/40-extension.yaml\\n```\\nAs we have the code and a method to trigger its execution available now, we can go ahead for a more interactive approach based on the [Delve debugger](https://github.com/go-delve/delve).\\nLet\'s start all over again and run our controller using Delve\\n```shell\\ndlv debug ./cmd/gardener-extension-mwe -- --kubeconfig=dev/kubeconfig.yaml  --ignore-operation-annotation=true --leader-election=false\\n```\\nand we will end up in a commandline with a `(dlv)` prompt.\\nNext, we ask `dlv` to break in the `Reconcile` method\\n``` shell\\n(dlv) b github.com/23technologies/gardener-extension-mwe/pkg/controller/lifecycle.(*actuator).Reconcile\\n```\\nand continue the execution of the controller\\n``` shell\\n(dlv) c\\n```\\nAfterwards, you should observe some output of the controller, again.\\nHowever, Delve will not break the execution until the `Reconcile` method is called.\\nThus, we apply the `Extension` resource once again\\n``` shell\\nkubectl apply -f example/40-extension.yaml\\n```\\nand Delve will stop in the `Reconcile` method.\\nNow, you can step through the code, see where it enters code paths pointing into the vendor directory, and inspect the values of certain variables.\\nObviously, the amount of variables you can inspect is limited in the MWE, but e.g. we can have a look at the `*extensionsv1alpha1.Extension` passed to the `Reconcile` method\\n``` shell\\n(dlv) p ex.ObjectMeta.Name\\n```\\nwhich should print `\\"mwe\\"`.\\nGenerally, this is a great way to approach unknown software, since you will quickly get a feeling for the different components.\\nThus, we expect that you can benefit from this workflow, when developing your own extensions.\\nEven though this approach offers capabilities for rapid prototyping, it is still limited, since we cannot act e.g. on `Shoot` clusters as available in a real world Gardener.\\nTherefore, we step into the development in a Gardener environment in the next section.\\n\\n### Development in a real Gardener environment\\nDeveloping and testing our extension in a real world Gardener requires a `ControllerRegistration` resource in the `Garden` cluster causing the installation of the controller in `Seed` clusters.\\nGenerally, the installation performed by Helm charts and consequently, we need to provide these charts in the repository.\\nAlso for the MWE, we prepared the `charts` directory containing all relevant Helmcharts for the deployment of the controller.\\nNote that this set of charts is very limited and in production scenarios you might want to add something like a `VerticalPodAutoscaler` as done e.g. in the [gardener-extension-shoot-networking-filter](https://github.com/gardener/gardener-extension-shoot-networking-filter).\\nHowever, implementing production ready charts goes beyond the scope of this post, and consequently the MWE charts were added in [another commit](https://github.com/23technologies/gardener-extension-mwe/commit/beee274314ac628c65a3dcb4846dad577744b36a).\\nThese charts target at running the controller in `Seed` clusters.\\nThus, in `charts/gardener-extension-mwe/values.yaml`, the image for the deployment is defined.\\nHowever, we do not want to push that image to a public container registry for each and every change we make to our code.\\nMoreover, we want to run the controller on our local machine for development purposes.\\nTherefore, we need to tweak the values before generating the `controller-registration.yaml`.\\nLet\'s go through it step by step:\\n``` shell\\ngit clone https://github.com/23technologies/gardener-extension-mwe.git\\ncd gardener-extension-mwe\\nmkdir dev\\ncp PATH-TO/KUBECONFIG-FOR-SEED.yaml dev/kubeconfig.yaml\\n```\\nNext, we generate the `controller-registration.yaml`, such that the controller is not deployed to the seed cluster and we can hook-in the controller running locally.\\nIn particular, we set `replicaCount=0` and `ignoreResources=true` in `./charts/gardener-extension-mwe/values.yaml`, before generating the `controller-registration.yaml`:\\n``` shell\\nyq eval -i \'.replicaCount=0 | .ignoreResources=true\' charts/gardener-extension-mwe/values.yaml\\n./vendor/github.com/gardener/gardener/hack/generate-controller-registration.sh mwe charts/gardener-extension-mwe v0.0.1 example/controller-registration.yaml Extension:mwe\\n```\\nNow, let\'s deploy the generated `controller-registration.yaml` into the `Garden` cluster:\\n``` shell\\nexport KUBECONFIG=PATH-TO/GARDEN-CLUSTER-KUBECONFIG.yaml\\nkubectl apply -f example/controller-registration.yaml\\n```\\nFrom now on, `Extension` resources of the type `mwe` will be deployed to `Seed` clusters when new `Shoot` clusters with\\n``` yaml\\n---\\napiVersion: core.gardener.cloud/v1beta1\\nkind: Shoot\\nmetadata:\\n  name: bar\\n  namespace: garden-foo\\nspec:\\n  extensions:\\n  - type: mwe\\n...\\n```\\nare created.\\nIn our controller, the `Reconcile` method will be triggered, when these `Extension` resources are reconciled.\\nTherefore, we can run the extension controller with Delve now\\n``` shell\\ndlv debug ./cmd/gardener-extension-mwe -- --kubeconfig=dev/kubeconfig.yaml  --ignore-operation-annotation=true --leader-election=false --gardener-version=\\"v1.44.4\\"\\n```\\nand we can perform debugging operations as explained above.\\nRemember, Delve will not break the execution until the `Reconcile` method is called.\\nNow, Gardener will create `Extension` resources for `Shoot`s which will trigger the `Reconcile` method of our controller.\\nConsequently, we need a new terminal pane in the repository root and execute\\n```shell\\nexport KUBECONFIG=PATH-TO/GARDEN-CLUSTER-KUBECONFIG.yaml\\nkubectl apply -f example/50-shoot.yaml\\n```\\nNote that it will take some time until the corresponding `Extension` resource will be created in the `Seed` cluster.\\nHang on tight and wait for the `Reconcile` method being executed.\\nYou can start investigating where your code goes using Delve now.\\nHappy hacking!\\n\\n## Last words\\nThis blog post shares our experience, when getting started with Gardener extension development.\\nWe hope that this contribution helps you get started more quickly than us.\\nIf you have any comment or ideas for improvements, do not hesitate to contact us.\\nWe are always willing to improve our work."},{"id":"gardener-ext-shoot-flux","metadata":{"permalink":"/blog/gardener-ext-shoot-flux","editUrl":"https://github.com/23technologies/23ke-docs/tree/main/blog/2022-06-08-gardener-ext-shoot-flux.md","source":"@site/blog/2022-06-08-gardener-ext-shoot-flux.md","title":"A Gardener Extension for universal Shoot Configuration","description":"TLDR;","date":"2022-06-08T00:00:00.000Z","formattedDate":"June 8, 2022","tags":[{"label":"gardener","permalink":"/blog/tags/gardener"},{"label":"gardener-extensions","permalink":"/blog/tags/gardener-extensions"}],"readingTime":6.55,"hasTruncateMarker":false,"authors":[{"name":"Jens Schneider","title":"software engineer","url":"https://github.com/jensac","imageURL":"https://github.com/jensac.png","key":"jensac"}],"frontMatter":{"slug":"gardener-ext-shoot-flux","title":"A Gardener Extension for universal Shoot Configuration","authors":"jensac","tags":["gardener","gardener-extensions"]},"prevItem":{"title":"Getting started with Gardener extension development","permalink":"/blog/gardener-ext-dev"}},"content":"## TLDR;\\nRecently, we developed the [gardener-extension-shoot-flux](https://github.com/23technologies/gardener-extension-shoot-flux), which enables preconfiguring `Shoot` clusters.\\nIf you want to give it a try, go and checkout the repository on Github.\\nIf you want to learn more, keep on reading.\\n\\n\x3c!-- markdown-toc start - Don\'t edit this section. Run M-x markdown-toc-refresh-toc --\x3e\\n**Table of Contents**\\n\\n- [TLDR;](#tldr)\\n- [Introduction](#introduction)\\n- [Example use cases](#example-use-cases)\\n    - [Development](#development)\\n    - [CI/CD](#cicd)\\n- [General concept](#general-concept)\\n- [Example Usage](#example-usage)\\n\\n\x3c!-- markdown-toc end --\x3e\\n\\n## Introduction\\n[Flux](https://fluxcd.io/) offers a set of controllers allowing for reconciling a Kubernetes cluster with a declarative state defined in e.g. a Git repository.\\nThus it enables GitOps workflows for Kubernetes clusters.\\nMoreover, it provides a general approach of deploying software components into Kubernetes clusters.\\n[Gardener](https://gardener.cloud/) is a multi cloud managed Kubernetes service allowing end users to create clusters with a few clicks in its dashboard.\\nHowever, the user will obtain a vanilla Kubernetes cluster and has to take care for all the components to be deployed into it.\\nOf course, the deployment can be performed manually by applying Kubernetes manifests to the cluster.\\nOn the other hand, tools like Flux can help to keep track of the deployments and automate the overall process.\\nThus, the combination of Gardener and Flux features the potential of creating new Kubernetes clusters in a pre-defined state.\\nFor the end users, this results in the seamless creation of clusters with all components on their wish list installed.\\nThe [gardener-extension-shoot-flux](https://github.com/23technologies/gardener-extension-shoot-flux) bridges the gap between Gardener and Flux and allows for reconciliation of `Shoot` clusters to resources defined in a Git repository.\\nBy concept, the extension operates on a per-project basis so that clusters in different projects can be reconciled to different repositories.\\n\\nThe rest of this post is organized as follows:\\nFirst, we will review a few use cases for this extension.\\nFurther, the general concept of the extension is outlined, and finally we provide an example on how to use the extension.\\n\\n## Example use cases\\n\\n### Development\\nImagine you are developing software which will eventually run on a Kubernetes cluster in the public cloud.\\nMoreover, you and your colleagues want to be able to perform some end-to-end tests besides running your local test suite.\\nFor these end-to-end test, an environment mimicking the final production environment is required.\\nTherefore, you might need tools like [cert-manager](https://cert-manager.io/) or [MinIO](https://min.io/).\\nHowever, you do not want keep several testing clusters in the public cloud available for economic reasons and, in consequence, you need to create new clusters on demand.\\nIn this case, the [gardener-extension-shoot-flux](https://github.com/23technologies/gardener-extension-shoot-flux) comes handy, since it allows to configure the cluster asynchronously.\\nPut simply, you can define the desired state of your cluster in a Git repository, and the new clusters will be reconciled to this state automatically.\\nEventually, this will save the effort to configure the clusters each and every time manually.\\nOf course, you could achieve something similar by hibernation of the development clusters.\\nHowever, in that case you are less flexible, since throwing away the cluster in case you lost track of your clusters state comes at the price of reconfiguring the entire cluster.\\n\\n### CI/CD\\nSimilar to the development use case above, you might want to run your CI/CD pipeline in Kubernetes clusters coming with a few components already installed.\\nAs your pipeline runs frequently, you want to create clusters on the fly or maybe pre-spawn just a few of them.\\nIn order to keep your pipeline simple, you can use the [gardener-extension-shoot-flux](https://github.com/23technologies/gardener-extension-shoot-flux) for the configuration of your CI/CD clusters.\\nThis way your pipeline can focus on the actual action and does not have to perform the cluster configuration beforehand.\\nThis most probably results in cleaner and more stable CI/CD pipelines.\\n\\n## General concept\\nThe general concept of this extension is visualized in the block diagram below.\\n```\\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n                 \u2502 Gardener operator                                       \u2502\\n                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n                 \u2502 - A human being                                         \u2502\\n                 \u2502                                                         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n                 \u2502                                                         \u2502            \u2502\\n                 \u2502                                                         \u2502            \u2502\\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\\n                          \u2502                           \u25b2                                 \u2502configures\\n                          \u2502deploys                    \u2502                                 \u2502SSH-key\\n                          \u2502Configmap                  \u2502read SSH-key                     \u2502\\n                          \u2502                           \u2502                                 \u2502\\n                          \u25bc                           \u2502                                 \u2502\\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\\n                 \u2502 Garden cluster                                         \u2502             \u2502\\n                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2524             \u2502\\n                 \u2502 Projetct 1             \u2502 Project 2               \u2502 ... \u2502             \u25bc\\n                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n                 \u2502- Configmap containing  \u2502- Configmap containing   \u2502     \u2502  \u2502 Git repository      \u2502\\n                 \u2502  flux configuration    \u2502  flux configuration     \u2502     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n                 \u2502                        \u2502                         \u2502     \u2502  \u2502 - Configuration for \u2502\\n            \u250c\u2500\u2500\u2500\u25ba\u2502- ControllerRegistration\u2502- ControllerRegistration \u2502 ... \u2502  \u2502   shoot clusters    \u2502\\n            \u2502    \u2502                        \u2502                         \u2502     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n            \u2502    \u2502- Shoot with extension  \u2502- Shoot with extension   \u2502     \u2502             \u25b2\\n            \u2502    \u2502  enabled               \u2502  enabled                \u2502     \u2502             \u2502\\n            \u2502    \u2502                        \u2502                         \u2502     \u2502             \u2502\\nread config \u2502    \u2502                        \u2502                         \u2502     \u2502             \u2502\\nand generate\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518             \u2502reconcile\\nSSH-keys    \u2502                                                                           \u2502\\n            \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\\n            \u2502    \u2502 Seed cluster           \u2502     \u2502 Shoot cluster          \u2502              \u2502\\n            \u2502    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524              \u2502\\n            \u2502    \u2502- Controller watching   \u2502     \u2502                        \u2502              \u2502\\n            \u2514\u2500\u2500\u2500\u2500\u253c\u2500 extension resource    \u2502     \u2502- Flux controllers  \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n                 \u2502     \u2502                  \u2502     \u2502                        \u2502\\n                 \u2502     \u2502deploys           \u2502     \u2502- GitRepository resource\u2502\\n                 \u2502     \u2502                  \u2502     \u2502                        \u2502\\n                 \u2502     \u25bc                  \u2502     \u2502- A main kustomization  \u2502\\n                 \u2502- Managed resources     \u2502     \u2502                        \u2502\\n                 \u2502  for flux controllers  \u2502     \u2502                        \u2502\\n                 \u2502  and flux config       \u2502     \u2502                        \u2502\\n                 \u2502                        \u2502     \u2502                        \u2502\\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\nAs depicted, the Gardener operator needs to deploy a `ConfigMap` into the Garden cluster.\\nThis `ConfigMap` holds some configuration parameters for the extension controller.\\nMoreover, the Gardener operator needs to configure an SSH-key for the Git repository in case of a private repository.\\nThis key can be read from the `Secret` called `flux-source` in the Garden cluster which is created by the extension controller.\\nOf course, the process of adding the SSH-key to the repository depends on the repository host.\\nE.g. for repositories hosted on Github, the key can simply be added as \\"Deploy key\\" in the web-interface.\\n\\nThe extension controller is running in `Seed` clusters.\\nBesides generating `Secret`s containing SSH-keys, it reads the configuration from the Garden cluster and creates `Managedresources` to be processed by the [Gardener Resource Manager](https://gardener.cloud/docs/gardener/concepts/resource-manager/#managedresource-controller).\\nThese `Managedresources` entail the resources for the Flux controllers, a [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) resource matching the configuration, and a main [Kustomization](https://fluxcd.io/docs/components/kustomize/kustomization/) resource.\\nOnce the Gardener Resource Manager has deployed these resources to the `Shoot` cluster, the Flux controllers will reconcile the cluster to the state defined in the Git repository.\\n\\nYou might wonder how the communication between `Seed` clusters and Garden cluster is established.\\nThis is achieved by making use of the `Secret` containing the `gardenlet-kubeconfig` which should be available, when the gardenlet is run inside the `Seed` cluster.\\nMost probably, this is not the most elegant solution, but it resulted in a quick first working solution.\\n\\n## Example Usage\\nOf course, you need to install the extension before you can use it.\\nYou can find `ControllerRegistration`s on our [Github release page](https://github.com/23technologies/gardener-extension-shoot-flux/releases).\\nSo, you can simply go for\\n``` shell\\nexport KUBECONFIG=KUBECONFIG-FOR-GARDEN-CLUSTER\\nkubectl -f https://github.com/23technologies/gardener-extension-shoot-flux/releases/download/v0.1.2/controller-registration.yaml\\n```\\nin order to install the extension.\\n\\nFor an exemplary use of the extension, we prepared a public repository containing manifest for the installation of [Podinfo](https://github.com/stefanprodan/podinfo).\\nAs a Gardener operator you can apply the following `ConfigMap` to your Garden cluster\\n``` yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: flux-config\\n  namespace: YOUR-PROJECT-NAMESPACE\\ndata:\\n  fluxVersion: v0.29.5 # optional, if not defined the latest release will be used\\n  repositoryUrl: https://github.com/23technologies/shootflux.git\\n  repositoryBranch: main\\n  repositoryType: public\\n```\\nAs the repository is public you can create a new `Shoot` now and enable the extension for this `Shoot`.\\nTake the snipped below as an example.\\n``` yaml\\napiVersion: core.gardener.cloud/v1beta1\\nkind: Shoot\\nmetadata:\\n  name: bar\\n  namespace: garden-foo\\nspec:\\n  extensions:\\n  - type: shoot-flux\\n...\\n```\\nGardener will take care for the `Shoot` creation process.\\nAs soon as you can, you can fetch the `kubeconfig.yaml` for your new `Shoot` from e.g. the Gardener dashboard.\\nNow, you can watch this cluster by\\n``` shell\\nexport KUBECONFIG=KUBECONFIG-FOR-SHOOT\\nk9s\\n```\\nand you should see that a `podinfo` deployment should come up.\\nGreat! You successfully created a `Shoot` with the [gardener-extension-shoot-flux](https://github.com/23technologies/gardener-extension-shoot-flux)."}]}')}}]);